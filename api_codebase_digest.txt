Codebase Analysis for: /Users/nathanielshawe/Desktop/python_projects/Neural_Network/api

Directory Structure:
└── api
    ├── train_runner.py
    ├── __init__.py
    ├── api.py
    └── predict_runner.py

Summary:
Total files analyzed: 4
Total directories analyzed: 0
Estimated output size: 7.84 KB
Actual analyzed size: 6.48 KB
Total tokens: 1509
Actual text content size: 6.47 KB

File Contents:

==================================================
File: api/train_runner.py
==================================================
import numpy as np
from models.network import NeuralNetwork #import the entire object with all of its functions (training, testing, etc)
from utils.config import MODES
from utils.winit import random_init, xavier_init, he_init

WEIGHT_INITS = {
    1: random_init,
    2: xavier_init,
    3: he_init,
}

def run_training_from_api(
    input_size,
    output_size,
    hidden_size,
    num_layers,
    dropout,
    optimizer_choice,
    mode_id,
    batch_size,
    learning_rate,
    epochs,
    data,
    labels,
    save_after_train=False,
    filename="latest_model.npz"
):
    # Get correct config and init function
    config = MODES[mode_id] #based on what user chose
    init_fn = WEIGHT_INITS[1]  # Default to random
    if mode_id in WEIGHT_INITS: 
        init_fn = WEIGHT_INITS[mode_id]

    # One-hot encode if needed
    if mode_id == 5: #if multi-class 
        def to_one_hot(index, num_classes):  #encode data in away computer can read
            one_hot = np.zeros(num_classes)
            one_hot[int(index)] = 1.0
            return one_hot
        labels = [to_one_hot(label, output_size) for label in labels] #new labels is the ones that have been encoded

    # Normalize if needed
    data = np.array(data, dtype=np.float64)
    labels = np.array(labels, dtype=np.float64)

    if config.get("normalize"): #if the mode the user chose calls for normalization, then normalize the data
        data = data / 10.0

    # Create and train model
    network = NeuralNetwork(
        input_size=input_size,
        hidden_size=hidden_size,
        num_layers=num_layers,
        output_size=output_size,
        config=config,
        dropout_rate=dropout,
        init_fn=init_fn,
        optimizer_choice=optimizer_choice
    )

    # Force 2D shape if labels are flat
    if isinstance(labels[0], (int, float)):
        # Convert to one-hot using np.eye
        num_classes = int(np.max(labels)) + 1
        labels = np.eye(num_classes)[np.array(labels).astype(int)].tolist()
    network.train(data, labels, learn_rate=learning_rate, epochs=epochs, bsize=batch_size)

    if save_after_train: #if autosave is on
        network.save_model(
            filename,
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout_rate=dropout,
            optimizer_choice=optimizer_choice,
            mode_id=mode_id,
            bsize=batch_size
        )
    
    return {
        "message": "Training complete",
        "samples": len(data),
        "epochs": epochs,
        "mode": mode_id,
        "output_size": output_size,
        "loss_history": network.loss_history,
        "accuracy_history": network.accuracy_history,
    }


==================================================
File: api/__init__.py
==================================================


==================================================
File: api/api.py
==================================================
from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.exceptions import RequestValidationError

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# now continue importing after CORS is applied
from .train_runner import run_training_from_api
from .predict_runner import run_prediction_from_api

from pydantic import BaseModel
from typing import List, Union, Optional
import uuid
import os


# 1. Define the structure of the expected input using a Pydantic mode
class TrainRequest(BaseModel):
    input_size: int
    output_size: int
    hidden_size: int
    num_layers: int
    dropout: float
    optimizer_choice: int
    mode_id: int
    batch_size: Optional[int] = None
    learning_rate: float
    epochs: int
    data: List[List[float]]
    labels: List[Union[float, List[float]]]
    save_after_train: Optional[bool] = False
    filename: Optional[str] = "latest_model.npz"

class PredictRequest(BaseModel):
    model_path: str
    test_data: List[List[float]]

# 2. Route for training
@app.post("/train")
def train_model(request: TrainRequest):
    training_id = str(uuid.uuid4())  # unique session ID
    result = run_training_from_api(
        input_size=request.input_size,
        output_size=request.output_size,
        hidden_size=request.hidden_size,
        num_layers=request.num_layers,
        dropout=request.dropout,
        optimizer_choice=request.optimizer_choice,
        mode_id=request.mode_id,
        batch_size=request.batch_size,
        learning_rate=request.learning_rate,
        epochs=request.epochs,
        data=request.data,
        labels=request.labels,
        save_after_train=request.save_after_train,
        filename=request.filename
    )
    return {
        "training_id": training_id,
        **result
    }

@app.post("/predict")
def predict(request: PredictRequest):
    try:
        result = run_prediction_from_api(
            model_path = request.model_path,
            test_data = request.test_data
        )
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Prediction failed: {str(e)}")


@app.get("/models")
def list_saved_models():
    try:
        models_dir = "saved_models"
        model_files = [f for f in os.listdir(models_dir) if f.endswith(".npz")]
        return {"models": model_files}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Could not list models: {str(e)}")

@app.get("/train_status")
def get_train_status(training_id: str):
    return {
        "training_id": training_id,
        "status": "complete",  # Always returns complete for now
        "message": "Training has finished. (Dummy response)"
    }

@app.exception_handler(Exception)
async def handle_general_error(request: Request, exc: Exception):
    print("❌ Backend error:", repr(exc))
    return JSONResponse(
        status_code=500,
        content={
            "error": "Something went wrong.",
            "details": str(exc)
        }
    )

==================================================
File: api/predict_runner.py
==================================================
import numpy as np
from utils.model_loader import load_full_model

def run_prediction_from_api(model_path, test_data):
    # Load model and config
    network, config = load_full_model(model_path)

    test_data = np.array(test_data, dtype=np.float64)
    if config.get("normalize"):
        test_data = test_data / 10.0

    predictions = []
    for sample in test_data:
        pred = network.feedforward(sample)
        if len(pred) == 1:
            predictions.append(float(pred[0]))  # Binary case
        else:
            predictions.append([float(p) for p in pred])  # Multiclass case

    return {
        "model": model_path,
        "num_samples": len(test_data),
        "predictions": predictions
    }

